{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape or not\n",
    "\n",
    "Uses a statistical method to determine how often certain pages are to be scraped using historical data.\n",
    "\n",
    "Exports the scrape rate to a .csv\n",
    "\n",
    "#### this data will be used:\n",
    "\n",
    "Page has a >50% new item rate -> always scrape\n",
    "\n",
    "Page has a <50% new item rate -> scrape in proportion to new item rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import historical data\n",
    "df = pd.DataFrame()\n",
    "for chunk in pd.read_csv(r\"fat_trimmer_data.csv\", chunksize=10000):\n",
    "    df = pd.concat([df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pages by if they have new items or not\n",
    "new_listings = df[df[\"NewItems\"] == True]\n",
    "no_new_listings = df[df[\"NewItems\"] == False]\n",
    "\n",
    "# Determine how many times each page has/hasn't new items\n",
    "new_listings_density = new_listings[\"PageNumber\"].value_counts()\n",
    "no_new_listings_density = no_new_listings[\"PageNumber\"].value_counts()\n",
    "\n",
    "# Make axis labels behave\n",
    "new_listings_density = new_listings_density.rename_axis(\"page\").rename(\"new_listings\")\n",
    "no_new_listings_density = no_new_listings_density.rename_axis(\"page\").rename(\"no_new_listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for both new and no new listings\n",
    "page_density = pd.concat([new_listings_density, no_new_listings_density], axis=1)\n",
    "# Fill NA with 0\n",
    "page_density.fillna(0, inplace=True)\n",
    "# Convert to int\n",
    "page_density = page_density.astype({\"new_listings\":\"int\", \"no_new_listings\":\"int\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page\n",
       "27      0.853503\n",
       "28      0.780255\n",
       "29      0.726115\n",
       "30      0.659236\n",
       "31      0.585987\n",
       "          ...   \n",
       "1811    0.000000\n",
       "1812    0.000000\n",
       "1814    0.000000\n",
       "1813    0.000000\n",
       "1816    0.000000\n",
       "Name: new_listing_rate, Length: 1816, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find new listing rate\n",
    "page_density[\"new_listing_rate\"] = page_density[\"new_listings\"] / (page_density[\"new_listings\"] + page_density[\"no_new_listings\"])\n",
    "page_density[\"new_listing_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "page_density[\"new_listing_rate\"].to_csv(r\"new_listing_rate.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
