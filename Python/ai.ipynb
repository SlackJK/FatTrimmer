{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SQL\n",
    "global con, cur\n",
    "with open(r\"sql_config.json\") as f:\n",
    "    sql_config = json.load(f)\n",
    "con = pyodbc.connect(\n",
    "    \"Driver={SQL Server Native Client 11.0};\"\n",
    "    f\"Server={sql_config['ip']},{sql_config['port']};\"\n",
    "    f\"Database={sql_config['database']};\"\n",
    "    f\"UID={sql_config['uid']};\"\n",
    "    f\"PWD={sql_config['password']}\"\n",
    ")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pull all data from FatTrimmerData into df\n",
    "df = pd.DataFrame()\n",
    "sql = \"SELECT * FROM FatTrimmerData;\"\n",
    "for chunk in pd.read_sql(sql, con, chunksize=10000):\n",
    "    df = pd.concat([df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where DeltaTime is -1\n",
    "df = df[df.DeltaTime != -1]\n",
    "# Drop any duplicate rows incase scraping makes a mistake\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Convert NewItems bool -> int\n",
    "df[\"NewItems\"] = df[\"NewItems\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create independent variables\n",
    "x = df.drop(columns=[\"NewItems\"])\n",
    "# Create dependent variables\n",
    "y = df[\"NewItems\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewItems</th>\n",
       "      <th>PageNumber</th>\n",
       "      <th>DeltaTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>4161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1054</td>\n",
       "      <td>3501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1326</td>\n",
       "      <td>7058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>4357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>660</td>\n",
       "      <td>2173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1237</td>\n",
       "      <td>2170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>3125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>6307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>4851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1784</td>\n",
       "      <td>1663000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NewItems  PageNumber  DeltaTime\n",
       "4          0         274    4161000\n",
       "5          0        1054    3501000\n",
       "8          0        1326    7058000\n",
       "9          0         114    4357000\n",
       "11         0         660    2173000\n",
       "12         0        1237    2170000\n",
       "13         0        1001    3125000\n",
       "14         0         560    6307000\n",
       "16         0          81    4851000\n",
       "19         0        1784    1663000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num physical devices: 1\n"
     ]
    }
   ],
   "source": [
    "# How many gpus can tensorflow see\n",
    "print(f\"num physical devices: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into training and testing (80-20 split)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "\n",
    "# Crate a sequential model with 3 layers, each layer is named after an anime waifu\n",
    "# Units describe the dimensionality of the layer output\n",
    "# Activation is the function responsible for computing the output of the node\n",
    "# TODO describe input shape bs\n",
    "model = Sequential([\n",
    "    Dense(units=1024, input_shape=(x_train.shape[1],), activation='sigmoid', name=\"Zero\"),\n",
    "    Dense(units=2048, activation='sigmoid', name=\"Rem\"),\n",
    "    Dense(units=1, activation='sigmoid', name=\"Megumin\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Zero (Dense)                (None, 1024)              3072      \n",
      "                                                                 \n",
      " Rem (Dense)                 (None, 2048)              2099200   \n",
      "                                                                 \n",
      " Megumin (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,104,321\n",
      "Trainable params: 2,104,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POINT OF NO RETURN\n",
    "\n",
    "Below is the code that runs the network, be careful.\n",
    "\n",
    "(see: 'I, Robot' starring Will Smith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (164203, 2)\n",
      "y_train shape: (164203,)\n",
      "x_train size: 328406\n",
      "y_train size: 164203\n",
      "Epoch 1/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 2/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 3/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 4/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 5/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 6/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 7/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 8/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 9/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 10/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 11/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 12/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 13/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 14/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 15/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 16/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 17/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 18/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 19/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 20/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 21/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 22/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 23/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 24/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 25/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 26/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 27/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 28/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 29/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 30/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 31/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 32/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 33/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 34/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 35/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 36/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 37/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 38/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 39/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 40/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 41/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 42/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 43/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 44/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 45/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 46/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 47/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 48/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 49/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 50/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 51/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 52/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 53/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 54/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 55/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 56/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 57/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 58/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 59/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 60/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 61/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 62/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 63/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 64/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 65/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 66/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 67/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 68/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 69/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 70/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 71/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 72/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 73/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 74/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 75/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 76/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 77/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 78/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 79/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 80/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 81/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 82/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 83/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 84/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 85/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 86/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 87/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 88/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 89/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 90/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 91/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 92/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 93/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 94/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 95/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 96/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 97/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 98/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 99/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 100/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 101/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 102/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 103/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 104/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 105/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 106/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 107/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 108/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 109/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 110/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 111/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 112/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 113/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 114/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 115/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 116/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 117/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 118/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 119/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 120/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 121/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 122/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 123/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 124/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 125/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 126/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 127/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 128/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 129/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 130/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 131/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 132/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 133/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 134/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 135/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 136/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 137/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 138/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 139/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 140/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 141/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 142/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0864 - accuracy: 0.9830\n",
      "Epoch 143/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 144/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 145/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 146/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 147/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 148/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 149/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 150/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 151/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 152/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 153/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0861 - accuracy: 0.9830\n",
      "Epoch 154/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 155/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 156/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 157/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 158/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 159/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 160/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 161/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 162/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 163/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 164/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 165/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 166/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 167/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 168/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 169/10000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0865 - accuracy: 0.9830\n",
      "Epoch 170/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 171/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 172/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 173/10000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 174/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0863 - accuracy: 0.9830\n",
      "Epoch 175/10000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0862 - accuracy: 0.9830\n",
      "Epoch 176/10000\n",
      "4/9 [============>.................] - ETA: 0s - loss: 0.0894 - accuracy: 0.9822"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\qsiba\\IdeaProjects\\FatTrimmer\\Python\\ai.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/qsiba/IdeaProjects/FatTrimmer/Python/ai.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_train size: \u001b[39m\u001b[39m{\u001b[39;00my_train\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/qsiba/IdeaProjects/FatTrimmer/Python/ai.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Epochs describe amount of times to iterate over x and y, more -> longer exec.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/qsiba/IdeaProjects/FatTrimmer/Python/ai.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/qsiba/IdeaProjects/FatTrimmer/Python/ai.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    602\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\qsiba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1126\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train size: {x_train.size}\")\n",
    "print(f\"y_train size: {y_train.size}\")\n",
    "# Epochs describe amount of times to iterate over x and y, more -> longer exec.\n",
    "model.fit(x_train, y_train, epochs=10000,batch_size=25000)\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
